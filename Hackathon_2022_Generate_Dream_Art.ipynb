{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagrutsharma/hack-2022/blob/main/Hackathon_2022_Generate_Dream_Art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "1p3_ycyZyT2Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hqGSA5bHEIl"
      },
      "source": [
        "# Import the libraries needed to start putting Google's Deep Dream together\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imutils\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi8VFyeCOdda"
      },
      "source": [
        "### Image Loader Function\n",
        "Load the image, resize it to a width of 350. We then swap the color channels from BGR to RGB, and return the image as a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3B3JqDIOd2X"
      },
      "source": [
        "def loadImage(imagePath):\n",
        "    '''returns the image pixel array resiezd to a width of 350'''\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = imutils.resize(image, width=350)\n",
        "    # convert from BGR to RGB\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = np.array(image)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL-9AG8WOhQP"
      },
      "source": [
        "### Deprocess Function \n",
        "Utility function to convert a tensor into a valid image. It \"undoes\" the preprocessing done for Inception and then casts the pixel values to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNepmFxgOfbO"
      },
      "source": [
        "def deprocess(image):\n",
        "    '''returns the deprocessed image'''\n",
        "    image = 255 * (image + 1.0) \n",
        "    image /= 2.0\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJiQuQriOoTt"
      },
      "source": [
        "### Calculate the loss after a single iteration of the DeepDream algorithm\n",
        "\n",
        "We firstly add a batch dimension to the image and then grab the activations, from specified layers of the Inception network after performing a forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fLK1GAYOkZr"
      },
      "source": [
        "def calculateLoss(image, model):\n",
        "    '''returns the sum of losses'''\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "    layerActivations = model(image)\n",
        "\n",
        "    # Create a list to store the intermediate losses\n",
        "    losses = []\n",
        "\n",
        "    # iterate over the layer activations\n",
        "    for act in layerActivations:\n",
        "        # compute the mean of each activation\n",
        "        loss = tf.reduce_mean(act)\n",
        "        # append these losses to the losses list\n",
        "        losses.append(loss)\n",
        "\n",
        "    # return the sum of the losses\n",
        "    return tf.reduce_sum(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbeicnOOOtVM"
      },
      "source": [
        "## The Deep Dream function\n",
        "\n",
        "This function performs a forward-pass and then taking the\n",
        "gradients and using them to update the input image to generate the deep dream effects.\n",
        "\n",
        "We input our model (inception network preferablly), the input image, the step size we use for our gradient updates and EPS, a tiny value used to prevent divide by zero errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd89e5hKOppR"
      },
      "source": [
        "# This is new addition to TF2.0. When we use the **tf.function** decorator to a function.\n",
        "# TensorFlow will automatically compile it into a graph which leads to faster execution.\n",
        "@tf.function\n",
        "def deepDreamSinglePass(model, image, stepSize, eps=1e-8):\n",
        "    ''' returns a tuple of the loss and the updated image'''\n",
        "\n",
        "    # Tells TenorFlow to record gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        # keep track of the image to calculate gradients and loss \n",
        "        tape.watch(image)\n",
        "        loss = calculateLoss(image, model)\n",
        "\n",
        "    # calculates the gradients of the loss with respect to the image\n",
        "    gradients = tape.gradient(loss, image)\n",
        "    # normalize the gradients \n",
        "    gradients /= tf.math.reduce_std(gradients) + eps \n",
        "    # K.maximum(K.mean(K.abs(grads)), K.epsilon())\n",
        "\n",
        "    # adjusts the image with the normalized gradients \n",
        "    image = image + (gradients * stepSize)\n",
        "    # clip its pixel values to the range [-1, 1]\n",
        "    image = tf.clip_by_value(image, -1, 1)\n",
        "\n",
        "    return (loss, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAgK7I3KOvdp"
      },
      "source": [
        "### Function that runs our Deep Dream Model\n",
        "\n",
        "Up to this point we can perform a single forward-pass of the DeepDream algorithm. We now need to design our training loop that enables us to do this for multiple iterations.\n",
        "\n",
        "The `fullDeepDreamModel` we will be making below will allow us to pass our model (inception) and input image for a specified number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjrdcPLmOrzR"
      },
      "source": [
        "def fullDeepDreamModel(model, image, iterations=50, stepSize=0.01):\n",
        "    '''runs the deep dream algorithm for a specified number of iterations \n",
        "    and returns the depresocessed image'''\n",
        "\n",
        "    # preprocess the image for the Inception network\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    # We iterate for the specified number of iterations\n",
        "    for iteration in range(iterations):\n",
        "        # use  our deepDreamSinglePass function to get the loss and the updated image\n",
        "        (loss, image) = deepDreamSinglePass(model, image, stepSize)\n",
        "\n",
        "        # print updates on the iteration progress\n",
        "        if iteration % 50 == 0:\n",
        "            print (\"Iteration {} with loss {}\".format(iteration, loss))\n",
        "\n",
        "    return deprocess(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHbfAHoNPHN3"
      },
      "source": [
        "#### Define the layers we are going to use for the dream\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53jHz8SEPFh6"
      },
      "source": [
        "# Chose the layers you'll be using \n",
        "# other available layers: \n",
        "# mixed8, mixed9_0, mixed9_1, mixed10\n",
        "# We try to get lower layer since they are resulting in more interesting art\n",
        "names = [\"mixed2\", \"mixed3\",\"mixed4\",\"mixed5\", \"mixed7\"]\n",
        "\n",
        "# define the octave scale and number of octaves (tweaking these values\n",
        "# will produce different output dreams)\n",
        "# OCTAVE_SCALE = 1.5 AND NUM_OCTAVES = 6 have generally given good results\n",
        "# NUM_OCTAVES will determine number of iterations. Keep it small if doing a demo.\n",
        "OCTAVE_SCALE = 1.5\n",
        "NUM_OCTAVES = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22wweGHFPKCn"
      },
      "source": [
        "### Execute Deep Dream\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3saPnc7EHVls"
      },
      "source": [
        "# get our input image\n",
        "\n",
        "sampleImage = loadImage('cchome-thanksgiving.jpg')\n",
        "print(\"Loaded Image\")\n",
        "\n",
        "# load the pre-trained Inception model from disk\n",
        "print(\"Loading Inception Model...\")\n",
        "baseModel = InceptionV3(include_top=False, weights=\"imagenet\")\n",
        "print(\"Loaded Inception Model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8PcVoyBMFba"
      },
      "source": [
        "# Explore the layers you can use by looking at the model summary\n",
        "# try using different mixed layers\n",
        "baseModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFaBiZRwUf1m"
      },
      "source": [
        "# build our dreaming model\n",
        "layers = [baseModel.get_layer(name).output for name in names]\n",
        "deepDreamModel = tf.keras.Model(inputs=baseModel.input, outputs=layers)\n",
        "\n",
        "# We convert the image to a TensorFlow constant for better performance,\n",
        "image = tf.constant(sampleImage)\n",
        "\n",
        "# We take the first two dimensions of the image and cast then them to float\n",
        "baseShape = tf.cast(tf.shape(image)[:-1], tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1zC1hhpPSXK"
      },
      "source": [
        "# Run our Deep Dream Generator!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5svaIpwPS7E"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We iterate over the number of Octaves (sizes) we are going to create using Deep Dream\n",
        "for n in range(NUM_OCTAVES):\n",
        "\n",
        "    # get the width and height for the current octave and cast them to integers\n",
        "    print(\"Working on octave {}\".format(n))\n",
        "    newShape = tf.cast(baseShape * (OCTAVE_SCALE ** n), tf.int32)\n",
        "\n",
        "    # Resize the image with new shape and convert it to numpy before running \n",
        "    # through our DeepDream Model\n",
        "    image = tf.image.resize(image, newShape).numpy()\n",
        "    image = fullDeepDreamModel(model=deepDreamModel, image=image, iterations=1000, stepSize=0.0001)\n",
        "\n",
        "# convert the final image to a numpy array and then save it to disk\n",
        "finalImage = np.array(image)\n",
        "cv2.imwrite('output.jpg', finalImage)\n",
        "\n",
        "plt.figure(figsize=(20,10)) \n",
        "plt.imshow(finalImage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}